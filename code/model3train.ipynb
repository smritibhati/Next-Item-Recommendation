{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/caser.py\", dst = \"../working/caser.py\")\ncopyfile(src = \"../input/evaluation.py\", dst = \"../working/evaluation.py\")\ncopyfile(src = \"../input/interactions.py\", dst = \"../working/interactions.py\")\ncopyfile(src = \"../input/utils.py\", dst = \"../working/utils.py\")\ncopyfile(src = \"../input/model2.py\", dst = \"../working/model2.py\")\ncopyfile(src = \"../input/model3.py\", dst = \"../working/model3.py\")\ncopyfile(src = \"../input/forget_mult.py\", dst = \"../working/forget_mult.py\")","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"'../working/forget_mult.py'"},"metadata":{}}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import argparse\nfrom time import time\n\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom caser import Caser\nfrom model2 import Model2\nfrom model3 import Model3\nfrom evaluation import evaluate_ranking\nfrom interactions import Interactions\nfrom utils import *\n\n\nclass Recommender(object):\n    \"\"\"\n    Contains attributes and methods that needed to train a sequential\n    recommendation model. Models are trained by many tuples of\n    (users, sequences, targets, negatives) and negatives are from negative\n    sampling: for any known tuple of (user, sequence, targets), one or more\n    items are randomly sampled to act as negatives.\n\n\n    Parameters\n    ----------\n\n    n_iter: int,\n        Number of iterations to run.\n    batch_size: int,\n        Minibatch size.\n    l2: float,\n        L2 loss penalty, also known as the 'lambda' of l2 regularization.\n    neg_samples: int,\n        Number of negative samples to generate for each targets.\n        If targets=3 and neg_samples=3, then it will sample 9 negatives.\n    learning_rate: float,\n        Initial learning rate.\n    use_cuda: boolean,\n        Run the model on a GPU or CPU.\n    model_args: args,\n        Model-related arguments, like latent dimensions.\n    \"\"\"\n\n    def __init__(self,\n                 n_iter=None,\n                 batch_size=None,\n                 l2=None,\n                 neg_samples=None,\n                 learning_rate=None,\n                 use_cuda=False,\n                 model_args=None):\n\n        # model related\n        self._num_items = None\n        self._num_users = None\n        self._net = None\n        self.model_args = model_args\n\n        # learning related\n        self._batch_size = batch_size\n        self._n_iter = n_iter\n        self._learning_rate = learning_rate\n        self._l2 = l2\n        self._neg_samples = neg_samples\n        self._device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n        # rank evaluation related\n        self.test_sequence = None\n        self._candidate = dict()\n\n    @property\n    def _initialized(self):\n        return self._net is not None\n\n    def _initialize(self, interactions):\n        self._num_items = interactions.num_items\n        self._num_users = interactions.num_users\n\n        self.test_sequence = interactions.test_sequences\n\n        self._net = Caser(self._num_users,\n                          self._num_items,\n                          self.model_args).to(self._device)\n\n        self._optimizer = optim.Adam(self._net.parameters(),\n                                     weight_decay=self._l2,\n                                     lr=self._learning_rate)\n\n    def fit(self, train, test, verbose=False):\n        \"\"\"\n        The general training loop to fit the model\n\n        Parameters\n        ----------\n\n        train: :class:`spotlight.interactions.Interactions`\n            training instances, also contains test sequences\n        test: :class:`spotlight.interactions.Interactions`\n            only contains targets for test sequences\n        verbose: bool, optional\n            print the logs\n        \"\"\"\n\n        # convert to sequences, targets and users\n        sequences_np = train.sequences.sequences\n        targets_np = train.sequences.targets\n        users_np = train.sequences.user_ids.reshape(-1, 1)\n\n        L, T = train.sequences.L, train.sequences.T\n\n        n_train = sequences_np.shape[0]\n\n        output_str = 'total training instances: %d' % n_train\n        print(output_str)\n\n        if not self._initialized:\n            self._initialize(train)\n\n        start_epoch = 0\n\n        for epoch_num in range(start_epoch, self._n_iter):\n\n            t1 = time()\n\n            # set model to training mode\n            self._net.train()\n\n            users_np, sequences_np, targets_np = shuffle(users_np,\n                                                         sequences_np,\n                                                         targets_np)\n\n            negatives_np = self._generate_negative_samples(users_np, train, n=self._neg_samples)\n\n            # convert numpy arrays to PyTorch tensors and move it to the corresponding devices\n            users, sequences, targets, negatives = (torch.from_numpy(users_np).long(),\n                                                    torch.from_numpy(sequences_np).long(),\n                                                    torch.from_numpy(targets_np).long(),\n                                                    torch.from_numpy(negatives_np).long())\n\n            users, sequences, targets, negatives = (users.to(self._device),\n                                                    sequences.to(self._device),\n                                                    targets.to(self._device),\n                                                    negatives.to(self._device))\n\n            epoch_loss = 0.0\n\n            for (minibatch_num,\n                 (batch_users,\n                  batch_sequences,\n                  batch_targets,\n                  batch_negatives)) in enumerate(minibatch(users,\n                                                           sequences,\n                                                           targets,\n                                                           negatives,\n                                                           batch_size=self._batch_size)):\n                items_to_predict = torch.cat((batch_targets, batch_negatives), 1)\n                items_prediction = self._net(batch_sequences,\n                                             batch_users,\n                                             items_to_predict)\n\n                (targets_prediction,\n                 negatives_prediction) = torch.split(items_prediction,\n                                                     [batch_targets.size(1),\n                                                      batch_negatives.size(1)], dim=1)\n\n                self._optimizer.zero_grad()\n                # compute the binary cross-entropy loss\n                positive_loss = -torch.mean(\n                    torch.log(torch.sigmoid(targets_prediction)))\n                negative_loss = -torch.mean(\n                    torch.log(1 - torch.sigmoid(negatives_prediction)))\n                loss = positive_loss + negative_loss\n\n                epoch_loss += loss.item()\n\n                loss.backward()\n                self._optimizer.step()\n\n            epoch_loss /= minibatch_num + 1\n\n            t2 = time()\n            if verbose and (epoch_num + 1) % 10 == 0:\n                precision, recall, mean_aps = evaluate_ranking(self, test, train, k=[1, 5, 10])\n                output_str = \"Epoch %d [%.1f s]\\tloss=%.4f, map=%.4f, \" \\\n                             \"prec@1=%.4f, prec@5=%.4f, prec@10=%.4f, \" \\\n                             \"recall@1=%.4f, recall@5=%.4f, recall@10=%.4f, [%.1f s]\" % (epoch_num + 1,\n                                                                                         t2 - t1,\n                                                                                         epoch_loss,\n                                                                                         mean_aps,\n                                                                                         np.mean(precision[0]),\n                                                                                         np.mean(precision[1]),\n                                                                                         np.mean(precision[2]),\n                                                                                         np.mean(recall[0]),\n                                                                                         np.mean(recall[1]),\n                                                                                         np.mean(recall[2]),\n                                                                                         time() - t2)\n                print(output_str)\n            else:\n                output_str = \"Epoch %d [%.1f s]\\tloss=%.4f [%.1f s]\" % (epoch_num + 1,\n                                                                        t2 - t1,\n                                                                        epoch_loss,\n                                                                        time() - t2)\n                print(output_str)\n\n    def _generate_negative_samples(self, users, interactions, n):\n        \"\"\"\n        Sample negative from a candidate set of each user. The\n        candidate set of each user is defined by:\n        {All Items} \\ {Items Rated by User}\n\n        Parameters\n        ----------\n\n        users: array of np.int64\n            sequence users\n        interactions: :class:`spotlight.interactions.Interactions`\n            training instances, used for generate candidates\n        n: int\n            total number of negatives to sample for each sequence\n        \"\"\"\n\n        users_ = users.squeeze()\n        negative_samples = np.zeros((users_.shape[0], n), np.int64)\n        if not self._candidate:\n            all_items = np.arange(interactions.num_items - 1) + 1  # 0 for padding\n            train = interactions.tocsr()\n            for user, row in enumerate(train):\n                self._candidate[user] = list(set(all_items) - set(row.indices))\n\n        for i, u in enumerate(users_):\n            for j in range(n):\n                x = self._candidate[u]\n                negative_samples[i, j] = x[\n                    np.random.randint(len(x))]\n\n        return negative_samples\n\n    def predict(self, user_id, item_ids=None):\n        \"\"\"\n        Make predictions for evaluation: given a user id, it will\n        first retrieve the test sequence associated with that user\n        and compute the recommendation scores for items.\n\n        Parameters\n        ----------\n\n        user_id: int\n           users id for which prediction scores needed.\n        item_ids: array, optional\n            Array containing the item ids for which prediction scores\n            are desired. If not supplied, predictions for all items\n            will be computed.\n        \"\"\"\n\n        if self.test_sequence is None:\n            raise ValueError('Missing test sequences, cannot make predictions')\n\n        # set model to evaluation model\n        self._net.eval()\n        with torch.no_grad():\n            sequences_np = self.test_sequence.sequences[user_id, :]\n            sequences_np = np.atleast_2d(sequences_np)\n\n            if item_ids is None:\n                item_ids = np.arange(self._num_items).reshape(-1, 1)\n\n            sequences = torch.from_numpy(sequences_np).long()\n            item_ids = torch.from_numpy(item_ids).long()\n            user_id = torch.from_numpy(np.array([[user_id]])).long()\n\n            user, sequences, items = (user_id.to(self._device),\n                                      sequences.to(self._device),\n                                      item_ids.to(self._device))\n\n            out = self._net(sequences,\n                            user,\n                            items,\n                            for_pred=True)\n\n        return out.cpu().numpy().flatten()","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"parser = argparse.ArgumentParser()\n# data arguments\nparser.add_argument('--train_root', type=str, default='../input/train.txt')\nparser.add_argument('--test_root', type=str, default='../input/test.txt')\nparser.add_argument('--L', type=int, default=5)\nparser.add_argument('--T', type=int, default=1)\n# train arguments\nparser.add_argument('--n_iter', type=int, default=50)\nparser.add_argument('--seed', type=int, default=1234)\nparser.add_argument('--batch_size', type=int, default=512)\nparser.add_argument('--learning_rate', type=float, default=1e-3)\nparser.add_argument('--l2', type=float, default=1e-6)\nparser.add_argument('--neg_samples', type=int, default=3)\nparser.add_argument('--use_cuda', type=str2bool, default=True)\n\nconfig = parser.parse_args([])\n\n# model dependent arguments\nmodel_parser = argparse.ArgumentParser()\nmodel_parser.add_argument('--d', type=int, default=128)\nmodel_parser.add_argument('--n_l', type=int, default=4)\nmodel_parser.add_argument('--nv', type=int, default=4)\nmodel_parser.add_argument('--nh', type=int, default=16)\nmodel_parser.add_argument('--drop', type=float, default=0.5)\nmodel_parser.add_argument('--ac_conv', type=str, default='sigm')\nmodel_parser.add_argument('--ac_fc', type=str, default='relu')\n\nmodel_config = model_parser.parse_args([])\nmodel_config.L = config.L\n\n# set seed\nset_seed(config.seed,\n         cuda=config.use_cuda)\n\n# load dataset\ntrain = Interactions(config.train_root)\n# transform triplets to sequence representation\ntrain.to_sequence(config.L, config.T)\n\ntest = Interactions(config.test_root,\n                    user_map=train.user_map,\n                    item_map=train.item_map)\n\nprint(config)\nprint(model_config)\n# fit model\nmodel = Recommender(n_iter=config.n_iter,\n                    batch_size=config.batch_size,\n                    learning_rate=config.learning_rate,\n                    l2=config.l2,\n                    neg_samples=config.neg_samples,\n                    model_args=model_config,\n                    use_cuda=config.use_cuda)\n\nmodel.fit(train, test, verbose=True)","execution_count":null,"outputs":[{"output_type":"stream","text":"Namespace(L=5, T=1, batch_size=512, l2=1e-06, learning_rate=0.001, n_iter=50, neg_samples=3, seed=1234, test_root='../input/test.txt', train_root='../input/train.txt', use_cuda=True)\nNamespace(L=5, ac_conv='sigm', ac_fc='relu', d=128, drop=0.5, n_l=4, nh=16, nv=4)\ntotal training instances: 771885\nEpoch 1 [36.7 s]\tloss=0.8422 [0.0 s]\nEpoch 2 [32.8 s]\tloss=0.6188 [0.0 s]\nEpoch 3 [32.8 s]\tloss=0.5202 [0.0 s]\nEpoch 4 [32.7 s]\tloss=0.4546 [0.0 s]\nEpoch 5 [32.8 s]\tloss=0.4079 [0.0 s]\nEpoch 6 [32.6 s]\tloss=0.3728 [0.0 s]\nEpoch 7 [32.6 s]\tloss=0.3452 [0.0 s]\nEpoch 8 [32.6 s]\tloss=0.3219 [0.0 s]\nEpoch 9 [32.6 s]\tloss=0.3012 [0.0 s]\nEpoch 10 [32.7 s]\tloss=0.2821, map=0.1681, prec@1=0.2924, prec@5=0.2469, prec@10=0.2212, recall@1=0.0184, recall@5=0.0748, recall@10=0.1292, [105.7 s]\nEpoch 11 [32.7 s]\tloss=0.2647 [0.0 s]\nEpoch 12 [32.6 s]\tloss=0.2500 [0.0 s]\nEpoch 13 [32.6 s]\tloss=0.2368 [0.0 s]\nEpoch 14 [32.6 s]\tloss=0.2252 [0.0 s]\nEpoch 15 [32.6 s]\tloss=0.2141 [0.0 s]\nEpoch 16 [32.8 s]\tloss=0.2060 [0.0 s]\nEpoch 17 [32.6 s]\tloss=0.1968 [0.0 s]\nEpoch 18 [32.6 s]\tloss=0.1896 [0.0 s]\nEpoch 19 [32.5 s]\tloss=0.1829 [0.0 s]\nEpoch 20 [32.6 s]\tloss=0.1773, map=0.1723, prec@1=0.2892, prec@5=0.2510, prec@10=0.2242, recall@1=0.0191, recall@5=0.0787, recall@10=0.1352, [105.6 s]\nEpoch 21 [32.6 s]\tloss=0.1724 [0.0 s]\nEpoch 22 [32.6 s]\tloss=0.1672 [0.0 s]\nEpoch 23 [32.5 s]\tloss=0.1629 [0.0 s]\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}