{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "# copy our file into the working directory (make sure it has .py suffix)\n",
    "# copyfile(src = \"../input/model1.py\", dst = \"../working/model1.py\")\n",
    "# copyfile(src = \"../input/evaluation.py\", dst = \"../working/evaluation.py\")\n",
    "# copyfile(src = \"../input/interactions.py\", dst = \"../working/interactions.py\")\n",
    "# copyfile(src = \"../input/utils.py\", dst = \"../working/utils.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from time import time\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from model1 import Model1\n",
    "from evaluation import evaluate_ranking\n",
    "from interactions import Interactions\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Recommender(object):\n",
    "    \"\"\"\n",
    "    Contains attributes and methods that needed to train a sequential\n",
    "    recommendation model. Models are trained by many tuples of\n",
    "    (users, sequences, targets, negatives) and negatives are from negative\n",
    "    sampling: for any known tuple of (user, sequence, targets), one or more\n",
    "    items are randomly sampled to act as negatives.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    n_iter: int,\n",
    "        Number of iterations to run.\n",
    "    batch_size: int,\n",
    "        Minibatch size.\n",
    "    l2: float,\n",
    "        L2 loss penalty, also known as the 'lambda' of l2 regularization.\n",
    "    neg_samples: int,\n",
    "        Number of negative samples to generate for each targets.\n",
    "        If targets=3 and neg_samples=3, then it will sample 9 negatives.\n",
    "    learning_rate: float,\n",
    "        Initial learning rate.\n",
    "    use_cuda: boolean,\n",
    "        Run the model on a GPU or CPU.\n",
    "    model_args: args,\n",
    "        Model-related arguments, like latent dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_iter=None,\n",
    "                 batch_size=None,\n",
    "                 l2=None,\n",
    "                 neg_samples=None,\n",
    "                 learning_rate=None,\n",
    "                 use_cuda=False,\n",
    "                 model_args=None):\n",
    "\n",
    "        # model related\n",
    "        self._num_items = None\n",
    "        self._num_users = None\n",
    "        self._net = None\n",
    "        self.model_args = model_args\n",
    "\n",
    "        # learning related\n",
    "        self._batch_size = batch_size\n",
    "        self._n_iter = n_iter\n",
    "        self._learning_rate = learning_rate\n",
    "        self._l2 = l2\n",
    "        self._neg_samples = neg_samples\n",
    "        self._device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "        # rank evaluation related\n",
    "        self.test_sequence = None\n",
    "        self._candidate = dict()\n",
    "\n",
    "    @property\n",
    "    def _initialized(self):\n",
    "        return self._net is not None\n",
    "\n",
    "    def _initialize(self, interactions):\n",
    "        self._num_items = interactions.num_items\n",
    "        self._num_users = interactions.num_users\n",
    "\n",
    "        self.test_sequence = interactions.test_sequences\n",
    "\n",
    "        self._net = Model1(self._num_users,\n",
    "                          self._num_items,\n",
    "                          self.model_args).to(self._device)\n",
    "\n",
    "        self._optimizer = optim.Adam(self._net.parameters(),\n",
    "                                     weight_decay=self._l2,\n",
    "                                     lr=self._learning_rate)\n",
    "\n",
    "    def fit(self, train, test, verbose=False):\n",
    "        \"\"\"\n",
    "        The general training loop to fit the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        train: :class:`spotlight.interactions.Interactions`\n",
    "            training instances, also contains test sequences\n",
    "        test: :class:`spotlight.interactions.Interactions`\n",
    "            only contains targets for test sequences\n",
    "        verbose: bool, optional\n",
    "            print the logs\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to sequences, targets and users\n",
    "        sequences_np = train.sequences.sequences\n",
    "        targets_np = train.sequences.targets\n",
    "        users_np = train.sequences.user_ids.reshape(-1, 1)\n",
    "\n",
    "        L, T = train.sequences.L, train.sequences.T\n",
    "\n",
    "        n_train = sequences_np.shape[0]\n",
    "\n",
    "        output_str = 'total training instances: %d' % n_train\n",
    "        print(output_str)\n",
    "\n",
    "        if not self._initialized:\n",
    "            self._initialize(train)\n",
    "\n",
    "        start_epoch = 0\n",
    "\n",
    "        for epoch_num in range(start_epoch, self._n_iter):\n",
    "\n",
    "            t1 = time()\n",
    "\n",
    "            # set model to training mode\n",
    "            self._net.train()\n",
    "\n",
    "            users_np, sequences_np, targets_np = shuffle(users_np,\n",
    "                                                         sequences_np,\n",
    "                                                         targets_np)\n",
    "\n",
    "            negatives_np = self._generate_negative_samples(users_np, train, n=self._neg_samples)\n",
    "\n",
    "            # convert numpy arrays to PyTorch tensors and move it to the corresponding devices\n",
    "            users, sequences, targets, negatives = (torch.from_numpy(users_np).long(),\n",
    "                                                    torch.from_numpy(sequences_np).long(),\n",
    "                                                    torch.from_numpy(targets_np).long(),\n",
    "                                                    torch.from_numpy(negatives_np).long())\n",
    "\n",
    "            users, sequences, targets, negatives = (users.to(self._device),\n",
    "                                                    sequences.to(self._device),\n",
    "                                                    targets.to(self._device),\n",
    "                                                    negatives.to(self._device))\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for (minibatch_num,\n",
    "                 (batch_users,\n",
    "                  batch_sequences,\n",
    "                  batch_targets,\n",
    "                  batch_negatives)) in enumerate(minibatch(users,\n",
    "                                                           sequences,\n",
    "                                                           targets,\n",
    "                                                           negatives,\n",
    "                                                           batch_size=self._batch_size)):\n",
    "                items_to_predict = torch.cat((batch_targets, batch_negatives), 1)\n",
    "                items_prediction = self._net(batch_sequences,\n",
    "                                             batch_users,\n",
    "                                             items_to_predict)\n",
    "\n",
    "                (targets_prediction,\n",
    "                 negatives_prediction) = torch.split(items_prediction,\n",
    "                                                     [batch_targets.size(1),\n",
    "                                                      batch_negatives.size(1)], dim=1)\n",
    "\n",
    "                self._optimizer.zero_grad()\n",
    "                # compute the binary cross-entropy loss\n",
    "                positive_loss = -torch.mean(\n",
    "                    torch.log(torch.sigmoid(targets_prediction)))\n",
    "                negative_loss = -torch.mean(\n",
    "                    torch.log(1 - torch.sigmoid(negatives_prediction)))\n",
    "                loss = positive_loss + negative_loss\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                self._optimizer.step()\n",
    "\n",
    "            epoch_loss /= minibatch_num + 1\n",
    "\n",
    "            t2 = time()\n",
    "            if verbose and (epoch_num + 1) % 10 == 0:\n",
    "                precision, recall, mean_aps = evaluate_ranking(self, test, train, k=[1, 5, 10])\n",
    "                output_str = \"Epoch %d [%.1f s]\\tloss=%.4f, map=%.4f, \" \\\n",
    "                             \"prec@1=%.4f, prec@5=%.4f, prec@10=%.4f, \" \\\n",
    "                             \"recall@1=%.4f, recall@5=%.4f, recall@10=%.4f, [%.1f s]\" % (epoch_num + 1,\n",
    "                                                                                         t2 - t1,\n",
    "                                                                                         epoch_loss,\n",
    "                                                                                         mean_aps,\n",
    "                                                                                         np.mean(precision[0]),\n",
    "                                                                                         np.mean(precision[1]),\n",
    "                                                                                         np.mean(precision[2]),\n",
    "                                                                                         np.mean(recall[0]),\n",
    "                                                                                         np.mean(recall[1]),\n",
    "                                                                                         np.mean(recall[2]),\n",
    "                                                                                         time() - t2)\n",
    "                print(output_str)\n",
    "            else:\n",
    "                output_str = \"Epoch %d [%.1f s]\\tloss=%.4f [%.1f s]\" % (epoch_num + 1,\n",
    "                                                                        t2 - t1,\n",
    "                                                                        epoch_loss,\n",
    "                                                                        time() - t2)\n",
    "                print(output_str)\n",
    "\n",
    "    def _generate_negative_samples(self, users, interactions, n):\n",
    "        \"\"\"\n",
    "        Sample negative from a candidate set of each user. The\n",
    "        candidate set of each user is defined by:\n",
    "        {All Items} \\ {Items Rated by User}\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        users: array of np.int64\n",
    "            sequence users\n",
    "        interactions: :class:`spotlight.interactions.Interactions`\n",
    "            training instances, used for generate candidates\n",
    "        n: int\n",
    "            total number of negatives to sample for each sequence\n",
    "        \"\"\"\n",
    "\n",
    "        users_ = users.squeeze()\n",
    "        negative_samples = np.zeros((users_.shape[0], n), np.int64)\n",
    "        if not self._candidate:\n",
    "            all_items = np.arange(interactions.num_items - 1) + 1  # 0 for padding\n",
    "            train = interactions.tocsr()\n",
    "            for user, row in enumerate(train):\n",
    "                self._candidate[user] = list(set(all_items) - set(row.indices))\n",
    "\n",
    "        for i, u in enumerate(users_):\n",
    "            for j in range(n):\n",
    "                x = self._candidate[u]\n",
    "                negative_samples[i, j] = x[\n",
    "                    np.random.randint(len(x))]\n",
    "\n",
    "        return negative_samples\n",
    "\n",
    "    def predict(self, user_id, item_ids=None):\n",
    "        \"\"\"\n",
    "        Make predictions for evaluation: given a user id, it will\n",
    "        first retrieve the test sequence associated with that user\n",
    "        and compute the recommendation scores for items.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        user_id: int\n",
    "           users id for which prediction scores needed.\n",
    "        item_ids: array, optional\n",
    "            Array containing the item ids for which prediction scores\n",
    "            are desired. If not supplied, predictions for all items\n",
    "            will be computed.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.test_sequence is None:\n",
    "            raise ValueError('Missing test sequences, cannot make predictions')\n",
    "\n",
    "        # set model to evaluation model\n",
    "        self._net.eval()\n",
    "        with torch.no_grad():\n",
    "            sequences_np = self.test_sequence.sequences[user_id, :]\n",
    "            sequences_np = np.atleast_2d(sequences_np)\n",
    "\n",
    "            if item_ids is None:\n",
    "                item_ids = np.arange(self._num_items).reshape(-1, 1)\n",
    "\n",
    "            sequences = torch.from_numpy(sequences_np).long()\n",
    "            item_ids = torch.from_numpy(item_ids).long()\n",
    "            user_id = torch.from_numpy(np.array([[user_id]])).long()\n",
    "\n",
    "            user, sequences, items = (user_id.to(self._device),\n",
    "                                      sequences.to(self._device),\n",
    "                                      item_ids.to(self._device))\n",
    "\n",
    "            out = self._net(sequences,\n",
    "                            user,\n",
    "                            items,\n",
    "                            for_pred=True)\n",
    "\n",
    "        return out.cpu().numpy().flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(L=5, T=1, batch_size=512, l2=1e-06, learning_rate=0.001, n_iter=50, neg_samples=3, seed=1234, test_root='datasets/ml1m/test/test.txt', train_root='datasets/ml1m/test/train.txt', use_cuda=False)\n",
      "Namespace(L=5, ac_conv='relu', ac_fc='relu', d=50, drop=0.5, nh=16, nv=4)\n",
      "total training instances: 44132\n",
      "Epoch 1 [14.0 s]\tloss=1.2378 [0.0 s]\n",
      "Epoch 2 [4.6 s]\tloss=1.0197 [0.0 s]\n",
      "Epoch 3 [5.5 s]\tloss=0.9530 [0.0 s]\n",
      "Epoch 4 [5.2 s]\tloss=0.9029 [0.0 s]\n",
      "Epoch 5 [5.4 s]\tloss=0.8585 [0.0 s]\n",
      "Epoch 6 [5.4 s]\tloss=0.8106 [0.0 s]\n",
      "Epoch 7 [5.3 s]\tloss=0.7670 [0.0 s]\n",
      "Epoch 8 [5.1 s]\tloss=0.7215 [0.0 s]\n",
      "Epoch 9 [4.8 s]\tloss=0.6805 [0.0 s]\n",
      "Epoch 10 [4.7 s]\tloss=0.6400, map=0.0124, prec@1=0.0094, prec@5=0.0085, prec@10=0.0071, recall@1=0.0029, recall@5=0.0134, recall@10=0.0213, [340.6 s]\n",
      "Epoch 11 [4.8 s]\tloss=0.6001 [0.0 s]\n",
      "Epoch 12 [4.9 s]\tloss=0.5662 [0.0 s]\n",
      "Epoch 13 [4.9 s]\tloss=0.5305 [0.0 s]\n",
      "Epoch 14 [4.9 s]\tloss=0.5054 [0.0 s]\n",
      "Epoch 15 [5.7 s]\tloss=0.4754 [0.0 s]\n",
      "Epoch 16 [6.2 s]\tloss=0.4513 [0.0 s]\n",
      "Epoch 17 [6.4 s]\tloss=0.4281 [0.0 s]\n",
      "Epoch 18 [6.2 s]\tloss=0.4031 [0.0 s]\n",
      "Epoch 19 [6.2 s]\tloss=0.3805 [0.0 s]\n",
      "Epoch 20 [6.1 s]\tloss=0.3615, map=0.0124, prec@1=0.0077, prec@5=0.0086, prec@10=0.0078, recall@1=0.0024, recall@5=0.0133, recall@10=0.0224, [339.2 s]\n",
      "Epoch 21 [5.9 s]\tloss=0.3394 [0.0 s]\n",
      "Epoch 22 [5.9 s]\tloss=0.3198 [0.0 s]\n",
      "Epoch 23 [5.9 s]\tloss=0.3027 [0.0 s]\n",
      "Epoch 24 [6.0 s]\tloss=0.2849 [0.0 s]\n",
      "Epoch 25 [6.0 s]\tloss=0.2706 [0.0 s]\n",
      "Epoch 26 [6.0 s]\tloss=0.2540 [0.0 s]\n",
      "Epoch 27 [5.9 s]\tloss=0.2427 [0.0 s]\n",
      "Epoch 28 [5.9 s]\tloss=0.2303 [0.0 s]\n",
      "Epoch 29 [5.9 s]\tloss=0.2168 [0.0 s]\n",
      "Epoch 30 [6.0 s]\tloss=0.2065, map=0.0124, prec@1=0.0090, prec@5=0.0083, prec@10=0.0071, recall@1=0.0032, recall@5=0.0127, recall@10=0.0206, [332.7 s]\n",
      "Epoch 31 [6.0 s]\tloss=0.1981 [0.0 s]\n",
      "Epoch 32 [5.9 s]\tloss=0.1878 [0.0 s]\n",
      "Epoch 33 [5.9 s]\tloss=0.1798 [0.0 s]\n",
      "Epoch 34 [6.1 s]\tloss=0.1709 [0.0 s]\n",
      "Epoch 35 [6.1 s]\tloss=0.1646 [0.0 s]\n",
      "Epoch 36 [5.8 s]\tloss=0.1560 [0.0 s]\n",
      "Epoch 37 [5.9 s]\tloss=0.1498 [0.0 s]\n",
      "Epoch 38 [5.9 s]\tloss=0.1433 [0.0 s]\n",
      "Epoch 39 [6.7 s]\tloss=0.1369 [0.0 s]\n",
      "Epoch 40 [6.3 s]\tloss=0.1329, map=0.0110, prec@1=0.0083, prec@5=0.0069, prec@10=0.0065, recall@1=0.0027, recall@5=0.0097, recall@10=0.0187, [336.2 s]\n",
      "Epoch 41 [6.0 s]\tloss=0.1298 [0.0 s]\n",
      "Epoch 42 [5.9 s]\tloss=0.1251 [0.0 s]\n",
      "Epoch 43 [5.9 s]\tloss=0.1189 [0.0 s]\n",
      "Epoch 44 [6.3 s]\tloss=0.1165 [0.0 s]\n",
      "Epoch 45 [6.3 s]\tloss=0.1103 [0.0 s]\n",
      "Epoch 46 [6.1 s]\tloss=0.1085 [0.0 s]\n",
      "Epoch 47 [6.0 s]\tloss=0.1045 [0.0 s]\n",
      "Epoch 48 [5.9 s]\tloss=0.1025 [0.0 s]\n",
      "Epoch 49 [6.0 s]\tloss=0.0979 [0.0 s]\n",
      "Epoch 50 [5.9 s]\tloss=0.0968, map=0.0111, prec@1=0.0092, prec@5=0.0071, prec@10=0.0065, recall@1=0.0030, recall@5=0.0106, recall@10=0.0188, [340.4 s]\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# data arguments\n",
    "parser.add_argument('--train_root', type=str, default='datasets/ml1m/test/train.txt')\n",
    "parser.add_argument('--test_root', type=str, default='datasets/ml1m/test/test.txt')\n",
    "parser.add_argument('--L', type=int, default=5)\n",
    "parser.add_argument('--T', type=int, default=1)\n",
    "# train arguments\n",
    "parser.add_argument('--n_iter', type=int, default=50)\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--batch_size', type=int, default=512)\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-3)\n",
    "parser.add_argument('--l2', type=float, default=1e-6)\n",
    "parser.add_argument('--neg_samples', type=int, default=3)\n",
    "parser.add_argument('--use_cuda', type=str2bool, default=False)\n",
    "\n",
    "config = parser.parse_args([])\n",
    "\n",
    "# model dependent arguments\n",
    "model_parser = argparse.ArgumentParser()\n",
    "model_parser.add_argument('--d', type=int, default=50)\n",
    "model_parser.add_argument('--nv', type=int, default=4)\n",
    "model_parser.add_argument('--nh', type=int, default=16)\n",
    "model_parser.add_argument('--drop', type=float, default=0.5)\n",
    "model_parser.add_argument('--ac_conv', type=str, default='relu')\n",
    "model_parser.add_argument('--ac_fc', type=str, default='relu')\n",
    "\n",
    "model_config = model_parser.parse_args([])\n",
    "model_config.L = config.L\n",
    "\n",
    "# set seed\n",
    "set_seed(config.seed,\n",
    "         cuda=config.use_cuda)\n",
    "\n",
    "\n",
    "filename = 'baby_pruned_1.csv'\n",
    "df = pd.read_csv(filename)\n",
    "userset = np.unique(df.iloc[:,1].values)\n",
    "itemset = np.unique(df.iloc[:,2].values)\n",
    "userdict = {userset[i]:i for i in range(len(userset))}\n",
    "itemdict = {itemset[i]:i+1 for i in range(len(itemset))}\n",
    "df['user_id']= df.apply(lambda row : userdict[row.userid],axis=1)\n",
    "df['item_id']= df.apply(lambda row : itemdict[row.itemid],axis=1)\n",
    "df = df.drop(columns=['userid','itemid'])\n",
    "\n",
    "randidx = np.random.rand(len(df)) < 0.8\n",
    "train_df = df[randidx]\n",
    "test_df = df[~randidx]\n",
    "\n",
    "train_df = train_df.sort_values(['user_id','timestamp'],ascending=[True,True])\n",
    "test_df = test_df.sort_values(['user_id','timestamp'],ascending=[True,True])\n",
    "\n",
    "# load dataset\n",
    "train = Interactions(train_df,userdict,itemdict)\n",
    "\n",
    "# transform triplets to sequence representation\n",
    "train.to_sequence(config.L, 1)\n",
    "\n",
    "test = Interactions(test_df,\n",
    "                    user_map=train.user_map,\n",
    "                    item_map=train.item_map)\n",
    "\n",
    "\n",
    "print(config)\n",
    "print(model_config)\n",
    "# fit model\n",
    "model = Recommender(n_iter=config.n_iter,\n",
    "                    batch_size=config.batch_size,\n",
    "                    learning_rate=config.learning_rate,\n",
    "                    l2=config.l2,\n",
    "                    neg_samples=config.neg_samples,\n",
    "                    model_args=model_config,\n",
    "                    use_cuda=config.use_cuda)\n",
    "\n",
    "model.fit(train, test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "predicted index: [16965   233 15992  2064  1888]\n",
      "predicted items: ['B00BU7404Y', 'B00005CAJK', 'B00A3YPG84', 'B000K50KOQ', 'B000I64CBY']\n",
      "actual indices: [   72  5851 10911 13445 15106]\n",
      "actual items : ['B000056C86', 'B00280UGFE', 'B004NONY8E', 'B006SFUEF2', 'B008KOC31W']\n"
     ]
    }
   ],
   "source": [
    "testcsr = test.tocsr()\n",
    "for user_id, row in enumerate(testcsr):\n",
    "    print(user_id)\n",
    "    predictions = model.predict(user_id=user_id)\n",
    "\n",
    "    predindices = predictions.argsort()\n",
    "    \n",
    "    print(\"predicted index:\",predindices[:5])\n",
    "    print(\"predicted items:\",[itemset[i-1] for i in predindices[:5]])\n",
    "    print(\"actual indices:\",row.indices)\n",
    "    print(\"actual items :\",[itemset[i-1] for i in row.indices])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
